\section{Type Inference Algorithm}
\label{sec:algorithm}

This section presents our type inference algorithm. This algorithm is
polymorphic with respect to the underlying pluggable typechecker: that is,
it applies equally well to any pluggable typechecker that performs flow-sensitive
local inference. The algorithm is described in two parts: first, the fixpoint
algorithm used to infer types for a particular program (\cref{sec:core-algorithm});
and second, the modifications we make to the pluggable typechecker to enable
that algorithm (\ie the \textsc{Instrument} function of \cref{alg:wpi-fixpoint},
in \cref{sec:instrument}).

\subsection{Fixpoint Algorithm}
\label{sec:core-algorithm}

\input{alg-wpi-fixpoint}

This section presents the core fixpoint algorithm, which appears
in \cref{alg:wpi-fixpoint}. The key idea is to iteratively analyze
the target program ($P$) with an instrumented version of the
pluggable typechecker, recording intermediate results at each
step (the sets of specifications $A$ and $A^{\prime}$) until
either the program can be typechecked (\ie $E \neq \emptyset$,
meaning there are no remaining typechecking errors) or the
specifications reach fixpoint (\ie $A = A^{\prime}$) and
no further refinement is possible.

This fixpoint loop is quite general: its success for our
purposes depends heavily on the \textsc{Instrument} procedure
that outputs locally-inferred specifications. \Cref{sec:instrument}
describes how we implement \textsc{Instrument} in a way that is
applicable to any pluggable typechecker.

\subsubsection{Soundness}
\label{sec:soundness}

Nevertheless, \cref{alg:wpi-fixpoint} is sound, assuming the underlying typechecker $T$
is sound. The key intuition is that \cref{alg:wpi-fixpoint} always runs $T$
on each set of proposed specifications. If any specification is incorrect,
$T$ will report an error---in the same manner as if a human had written an
incorrect specification. In particular, this means that the instrumentation
(\cref{sec:instrument}) has no obligation to produce annotations that are sound,
so long as the underlying typechecking algorithm itself is not modified.
\todo{Maybe formalize this a bit more?}

\subsection{Modifications to the Typechecker}
\label{sec:instrument}

The algorithm presented in \cref{sec:core-algorithm} works for
any pluggable typechecker that supports flow-sensitive inference:
that is, it does not require a type system implementer to write
any special rules to support type inference. Instead, this section
describes our approach to \emph{automatically} modify a given
pluggable typechecker to support inference, corresponding to the
\textsc{Instrument} helper function in \cref{alg:wpi-fixpoint}.

The key idea behind our approach to instrumenting the typechecker
is to do the instrumentation at the \emph{framework} level: that is
we modify the underlying infrastructure that typecheckers rely on
once, and the modifications can then be applied generically to any
typechecker compatible with the original framework. Our modifications
can be conceptualized at the type-qualifier-theory level: that is,
we modify the rules for typechecking \emph{any} pluggable type system
so that inference is supported,
regardless of the particular qualifiers it happens to support.

Our modifications rely on the fact that modern pluggable type systems
do \emph{local}, intra-procedural flow-sensitive type inference. This assumption is
reasonable in practice: programmers are generally not willing to
write type qualifiers within method bodies, and so 
the pluggable type frameworks that exist in practice \todo{all?}
support this feature~\cite{PapiACPE2008}~\todo{cite any other practical pluggable
  type frameworks that exist, if there are any}. \todo{Also mention that
  Java itself supports this now for the base type system?} \todo{Also mention
  that this seems to be a general trend in language design, citing maybe Kotlin?}

\begin{figure}
  \begin{mathpar}
    \inferrule* [right=INV]
                {
                  \std{\Gamma \vdash m(f_0,\ldots,f_n) : }~\qual{q_R}~\std{\tau_R}
                  \\
                  \std{\Gamma \vdash \forall i \in 0,\ldots,n,~e_i :~} \qual{q_{i_A}}~\std{\tau_{i_A}}
                  \\
                  \std{\Gamma \vdash \forall i \in 0,\ldots,n,~f_i :~} \qual{q_{i_F}}~\std{\tau_{i_F}}
                  \\
                  \std{\Gamma \vdash \forall i \in 0,\ldots,n,~} \qual{q_{i_A}}~\std{\tau_{i_A}}~\sqsubseteq~\qual{q_{i_F}}~\std{\tau_{i_F}}
                  \\
                  \infr{\infEnv \vdash \forall i \in 0,\ldots,n,~f_i~:~q_{i_I}~\tau_{i_F}}
                }
                {
                  \std{\Gamma \vdash m(e_0,\ldots,e_n) : }~\qual{q_R}~\std{\tau_R}
                  \\
                  \infr{\infEnv \vdash \forall i \in 0,\ldots,n,~f_i~:~\mathit{LUB_Q}(q_{i_A},~q_{i_I})~\tau_{i_F} }
                }
  \end{mathpar}
  \caption{Modified type rules used by our pluggable type framework. \std{Gray} indicates
    standard type rules for a Java-like language. \qual{Black} indicates additions to support
    pluggable typechecking. \infr{Red} indicates additions to support inference, \ie our
    contribution in this paper.}
  \label{fig:type-rules}
\end{figure}

The modified type rules appear in \cref{fig:type-rules}. \std{Gray} indicates
standard type rules for a Java-like language. \qual{Black} indicates additions to support
pluggable typechecking. \infr{Red} indicates additions to support inference, \ie our
contribution in this paper. To read these type rules, we need to define some terms.

$\infEnv$ is the \emph{inference environment}, similar to the standard (qualified)
typing environment $\Gamma$. $\Gamma$ maps expressions and declarations to qualified types.
$\infEnv$ has two key differences: it maps only declarations to possibly-qualified types.
$\infEnv$ only maps declarations because our inference procedure does not need to infer
types for expressions: in fact, we assume that $\Gamma$ already does so (via flow-sensitive
refinement). Rather, $\infEnv$'s purpose is to map declarations to the results of inference.
Unlike $\Gamma$, the values in $\infEnv$ are possibly-qualified, meaning that they can either
be qualified types or unqualified types. Initially, $\infEnv$ contains qualified types only
for declarations that were qualified before the current round of inference (which may come from
the programmer or from a previous inference round in the \cref{alg:wpi-fixpoint} fixpoint loop).
Once the current round of inference terminates, $\infEnv$ is used to produce the results of the
inference round by returning the set of all qualified types: any type that remains unqualified
throughout inference is not annotated, because no information about it was learned.

We define the function $\mathit{LUB_Q}(q_1, q_2)$ to account for possibly-qualified types.
$q_1$ and $q_2$ are each either a type qualifier or ``not present''.
If both arguments are qualifiers, then the result of $\mathit{LUB_Q}$
is just their least upper bound. If only one qualifier is present, then $\mathit{LUB_Q}$'s result
is that qualifier; if both qualifiers are not present, $\mathit{LUB_Q}$'s result is ``not present'',
resulting in an unqualified result (since calls $\mathit{LUB_Q}$ must always precede a ground type
from the original type system).

\todo{Create a figure with modified type rules for handling
  inference, with one rule for each part of \<WholeProgramInference>.
  We want to present this as a theoretical contribution: \ie, as
  a new way of building a pluggable type system. Maybe take the original
  rules from~\cite{FosterFFA99}?}

\todo{Then, this section should describe the interesting parts of the
  modified type rules in detail.}
