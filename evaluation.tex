\section{Evaluation}
\label{sec:evaluation}

We aim to answer two research questions:
\begin{itemize}
\item \textbf{RQ1:} Is our approach \emph{effective}?  Can it infer most of the
  annotations that humans have written in prior case studies of pluggable typecheckers?
\item \textbf{RQ2:} Is our approach \emph{general}?  Without any extra per-checker work,
  does it apply to many extant pluggable typecheckers?\todo{Or, maybe:
    how much work?}
\end{itemize}

\input{table-case-studies}

We answer both of these research questions with one set of
experiments.
%
Our methodology is to collect a set of
projects that have already been annotated for pluggable typechecking,
remove the human-written type annotations,
and then run type inference.
%
To answer \textbf{RQ1}, we compare the output of our approach to the
ground-truth human-written type annotations (\cref{sec:results}).
\todo{To a ``threats to validity'' section: the human-written type
  annotations might contain errors.}
%
\textbf{RQ2} is answered incidentally: \todo{no} modifications to
the overall approach were required to handle any of the \todo{number} typecheckers
we considered; we further discuss our approach's performance for different typecheckers
in \cref{sec:by-checker}.

\subsection{Methodology}
\label{sec:methodology}

We collected subject programs from two sources:
\begin{itemize}
\item a GitHub search for projects using the Checker Framework, and
\item case studies in the evaluations of papers about pluggable typecheckers built
  on the Checker Framework.
\end{itemize}

\todo{The following describes the experimental evaluation I want to do. I doubt we'll
  have this done completely by the time the first ISSTA deadline arrives on 10 November,
  so we may need to rewrite this section to just say that we sampled projects by convenience
  that obviously used the CF, i.e., plume-lib.}

To find projects using the Checker Framework on GitHub, we searched GitHub for characteristic
markers of Checker Framework integration in build scripts. For example, one of the search
queries we used was for application of the Checker Framework's Gradle plugin; we used similar
search queries for other common integrations and build systems, which we derived from
the Checker Framework manual.
\todo{What about looking for import statements?  I realize not all of those
run a pluggable type-checker, but it's still worth doing.}
Our goal was to find projects that actively used a checker---and
consequently had annotations that could serve as ground truth for our experiments---so
searching for build system integration guarantees that the projects that we found at least
have (attempted to) run a checker at some time in the past. This stage of our search
resulted in \todo{X} projects. We then filtered those projects by removing those that
either did not typecheck with the pluggable typechecker associated with their annotations (\todo{W}
projects) or contained no annotations (\todo{Z} projects),
resulting in a dataset of \todo{V} projects with ground-truth annotations.
Call this dataset \textbf{DS-GH}.

We augmented this dataset by adding projects that had been hand-annotated for checkers
described in prior work. We checked each paper that has cited the original paper
about the Checker Framework~\cite{PapiACPE2008} to determine whether it includes any
case studies of Checker Framework checkers that are publicly available. \todo{X} papers
claimed such case studies; we could locate, build, and typecheck \todo{Y} projects (of
the \todo{Z} case studies claimed in these papers); call the resulting dataset \textbf{DS-PW}.

Together, \textbf{DS-GH} and \textbf{DS-PW} contain \todo{X} projects, \todo{Y} human-written
annotations that we use as ground truth, and \todo{Z} lines of non-comment, non-blank Java
source code.

For each project in \textbf{DS-GH} and \textbf{DS-PW}, we performed the following steps:
\begin{enumerate}
\item remove the existing, ground-truth annotations,
\item modify the build system to run the typechecker(s) using our
  tool,\todo{Is this just adding the \<-Ainfer> command-line argument, or
    is it more?}
\item write a short script to implement the ``outer loop'' of the algorithm
  (\ie \cref{alg:wpi-fixpoint}) by repeatedly invoking the project's build
  system,\todo{Is this the same script for all projects, or does it differ
    per project?} and
\item run this script to fixpoint to collect the final set of candidate type annotations
  inferred by our approach.
\end{enumerate}
\todo{Does it make sense to build steps 3 and 4 into a script or into the framework?}

Finally, we compare the final set of annotations generated by our approach to the original,
human-written ground-truth annotations. Our scripts and data are available
at~\todo{artifact location}.

\subsection{Results: Effectiveness}
\label{sec:results}

Our main results appear in \cref{tab:case-studies}. \todo{Discuss them.}

\subsection{Results: Generality}
\label{sec:by-checker}

\input{table-checkers}

We also broke out the results by checker, in \cref{tab:checkers}. \todo{Discuss
  the results, especially if there are significant differences between checkers..}

\todo{Also emphasize the absolute number of checkers that WPI applies to.}
