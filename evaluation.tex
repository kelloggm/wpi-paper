\section{Evaluation}
\label{sec:evaluation}

We aim to answer two research questions:
\begin{itemize}
\item \textbf{RQ1:} Is iterated local type inference \emph{effective}?  Can it infer most of the
  annotations that humans have written in prior case studies of pluggable typecheckers?
\item \textbf{RQ2:} Is iterated local type inference \emph{easy to apply}
  to an existing type-checker?  How much per-checker work is required?
\end{itemize}

\input{table-case-studies}

\todo{Should we put the RQs in the other order?  It's necessary to
  \emph{have} an inferring type checker before using it.}


To answer \textbf{RQ1}, we collected
projects that have already been annotated for pluggable typechecking,
removed the human-written type annotations,
ran type inference,
and compared the inference output to the
ground-truth human-written type annotations (\cref{sec:results}).

To answer \textbf{RQ2}, we converted \numTypeSystems pluggable
type-checkers into inferring type-checkers, and recorded the effort required.


\subsection{Methodology}
\label{sec:methodology}

For each subject project, we performed the following steps:\todo{Keep this
  list short, probably.  Put extra details afterward.}

\begin{enumerate}
\item remove the existing, ground-truth annotations,\todo{Discuss leaving
  in \<@SuppressWarnings> annotations.  It's only fair because those
  indicate that the program is not type-checkable.  Is it interesting to
  try with those removed too?  I suspect not, and we don't have time for
  such an experiment.}
\item manually modify the build system to run the typechecker(s) using our
  tool,%\todo{Is this just adding the \<-Ainfer> command-line argument, or
    %is it more?} Typically it is more. See experiments/experimental-procedure.md
  \todo{Overview what is required.}
\item write a short script to implement the ``outer loop'' of the algorithm
  (\ie \cref{alg:wpi-fixpoint}) by repeatedly invoking the project's build
  system, %\todo{Is this the same script for all projects, or does it differ
  %per project?} It differs per project (though it's usually similar, and we do have a template).
    and
\item run this script to fixpoint.  Its output is the inferred type annotations.
\end{enumerate}
% \todo{Does it make sense to build steps 3 and 4 into a script or into the framework?}
% No. We tried that with wpi-many.sh, and it cannot build a lot of projects, due to the
% limitations of dljc. Sometimes it is necessary to do per-project work to enable the
% tool, and that's ok.

\todo{Exactly what measurements do we make?}
Finally, we compare the final set of annotations generated by our approach to the original,
human-written ground-truth annotations. Our scripts and data are available
at~\todo{artifact location}.


\subsection{Subject programs}
\label{sec:subject-programs}

We collected subject programs from two sources:
\begin{itemize}
\item a GitHub search for projects using the Checker Framework, and
\item case studies in the evaluations of papers about pluggable typecheckers built
  on the Checker Framework.
\end{itemize}

We searched GitHub for projects using the Checker Framework.  For example,
we searched for use of the Checker Framework's Gradle plugin, and similarly
for other common integrations with build systems, which we derived from
the Checker Framework manual.

We did not search for projects that use type annotations, such as
\<import org.checkerframework...>.  The Checker Framework's annotations,
such as \<@Nullable>, are the \emph{de facto} standard and are used by
other tools and for documentation, so there are hundreds of thousands of
hits for such a search.
Our goal was to find projects that run a pluggable type-checker and
whose annotations can serve as ground truth for our experiments.

Our search yielded \todo{X} projects.
Of these, \todo{V} typecheck with a pluggable typechecker.
\todo{Does this happen?  I would leave it out for simplicity if it is rare.
  ``contained no annotations (\todo{Z} projects)''.}
Call this dataset \textbf{DS-GH}.\todo{What is the point of naming the two
  datasets differently?  Do you think they may have different
  characteristics, that the experiments will compare?  This doesn't seem so
  interesting to me.  I suggest treating all the subject programs uniformly.}

We augmented this dataset by adding projects from the research literature.
We checked each paper that has cited the original paper
about the Checker Framework~\cite{PapiACPE2008}\todo{Some papers cite other
  publications, like the ICSE 2011 paper, or the manual.} to determine whether it includes any
case studies of Checker Framework checkers that are publicly available. \todo{X} papers
claimed such case studies; we could locate, build, and typecheck \todo{Y} projects (of
the \todo{Z} case studies claimed in these papers); call the resulting dataset \textbf{DS-PW}.

Together, \textbf{DS-GH} and \textbf{DS-PW} contain \todo{X} projects, \todo{Y} human-written
annotations that we use as ground truth, and \todo{Z} lines of non-comment, non-blank Java
source code.


\subsection{Results: Effectiveness}
\label{sec:results}

Our main results appear in \cref{tab:case-studies}. \todo{Discuss them.}


\subsection{Results: Generality}
\label{sec:by-checker}

\input{table-checkers}

We also broke out the results by checker, in \cref{tab:checkers}. \todo{Discuss
  the results, especially if there are significant differences between checkers..}

\todo{Also emphasize the absolute number of checkers that WPI applies to.}

% LocalWords:  RQ1 typecheckers RQ2 typechecking typecheck typechecker DS
% LocalWords:  GH Ainfer WPI
