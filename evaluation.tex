\section{Evaluation}
\label{sec:evaluation}

We aim to answer two research questions:
\begin{itemize}
\item \textbf{RQ1:} is our approach \emph{effective}: can infer most of the
  annotations that humans have written in prior case studies of pluggable typecheckers?
\item \textbf{RQ2:} is our approach \emph{general}: without any extra per-checker work,
  can we apply it to many extant pluggable typecheckers?.
\end{itemize}

\input{table-case-studies}

We answer both of these research questions with one set of
experiments.
%
Our methodology (at a high-level) is to collect a set of
projects that have been annotated by humans so that some pluggable
typechecker can typecheck them,
remove the human-written type annotations,
and then apply our approach.
%
To answer \textbf{RQ1}, we compare the output of our approach to the
ground-truth human-written type annotations (\cref{sec:results}}.
%
\textbf{RQ2} is answered incidentally: \todo{no} modifications to
the overall approach were required to handle any of the \todo{number} typecheckers
we considered; we further discuss our approach's performance for different typecheckers
in \cref{sec:by-checker}.

\subsection{Methodology}
\label{sec:methodology}

\todo{Write this.}

\subsection{Results: Effectiveness}
\label{sec:results}

Our main results appear in \cref{tab:case-studies}. \todo{Discuss them.}

\subsection{Results: Generality}
\label{sec:by-checker}

\input{table-checkers}

We also broke out the results by checker, in \cref{tab:checkers}. \todo{Discuss
  the results, especially if there are significant differences between checkers..}

\todo{Also emphasize the absolute number of checkers that WPI applies to.}
