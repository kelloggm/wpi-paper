\section{Evaluation}
\label{sec:evaluation}

We aim to answer two research questions:
\begin{itemize}
\item \textbf{RQ1:} is our approach \emph{effective}: can infer most of the
  annotations that humans have written in prior case studies of pluggable typecheckers?
\item \textbf{RQ2:} is our approach \emph{general}: without any extra per-checker work,
  can we apply it to many extant pluggable typecheckers?.
\end{itemize}

\input{table-case-studies}

We answer both of these research questions with one set of
experiments.
%
Our methodology (at a high-level) is to collect a set of
projects that have been annotated by humans so that some pluggable
typechecker can typecheck them,
remove the human-written type annotations,
and then apply our approach.
%
To answer \textbf{RQ1}, we compare the output of our approach to the
ground-truth human-written type annotations (\cref{sec:results}}.
%
\textbf{RQ2} is answered incidentally: \todo{no} modifications to
the overall approach were required to handle any of the \todo{number} typecheckers
we considered; we further discuss our approach's performance for different typecheckers
in \cref{sec:by-checker}.

\subsection{Methodology}
\label{sec:methodology}

We collected subject programs from two sources:
\begin{itemize}
\item a GitHub search for projects using the Checker Framework, and
\item case studies in the evaluations of papers about pluggable typecheckers built
  on the Checker Framework.
\end{itemize}

\todo{The following describes the experimental evaluation I want to do. I doubt we'll
  have this done completely by the time the first ISSTA deadline arrives on 10 November,
  so we may need to rewrite this section to just say that we sampled projects by convenience
  that obviously used the CF, i.e., plume-lib.}

To find projects using the Checker Framework on GitHub, we searched GitHub for characteristic
markers of Checker Framework integration in build scripts. For example, one of the search
queries we used was for application of the Checker Framework's Gradle plugin; we used similar
searche queries for other common integrations and build systems, which we derived from
the Checker Framework manual. Our goal was to find projects that actively used a checker---and
consequently had annotations that could serve as ground truth for our experiments---so
searching for build system integration guarantees that the projects that we found at least
have (attempted to) run a checker at some time in the past. This stage of our search
resulted in \todo{X} projects. We then filtered those projects by removing those that
either did not build at all (\todo{Y} projects), contained no annotations (\todo{Z} projects),
or did not typecheck with the pluggable typechecker associated with their annotations (\todo{W}
projects), resulting in a dataset of \todo{V} projects with ground-truth annotations.
Call this dataset \textbf{DS-GH}.

We augmented this dataset by adding projects that had been hand-annotated for checkers
described in prior work. We checked each paper that has cited the original paper
about the Checker Framework~\cite{PapiACPE2008} to determine whether it includes any
case studies of Checker Framework checkers that are publicly available. \todo{X} papers
claimed such case studies; we could locate, build, and typecheck \todo{Y} projects (of
the \todo{Z} case studies claimed in these papers); call the resulting dataset \textbf{DS-PW}.

Together, \textbf{DS-GH} and \textbf{DS-PW} contain \todo{X} projects, \todo{Y} human-written
annotations that we use as ground truth, and \todo{Z} lines of non-comment, non-blank Java
source code.

For each project in \textbf{DS-GH} and \textbf{DS-PW}, we performed the following steps:
\begin{enumerate}
\item remove the existing, ground-truth annotations,
\item modify the build system to run the typechecker(s) using our tool,
\item write a short script to implement the ``outer loop'' of the algorithm
  (\ie \cref{alg:wpi-fixpoint}) by repeatedly invoking the project's build system, and
\item run this script to fixpoint to collect the final set of candidate type annotations
  inferred by our approach.
\end{enumerate}

Finally, we compare the final set of annotations generated by our approach to the original,
human-written ground-truth annotations. Our scripts and data are available
at~\todo{artifact location}.

\subsection{Results: Effectiveness}
\label{sec:results}

Our main results appear in \cref{tab:case-studies}. \todo{Discuss them.}

\subsection{Results: Generality}
\label{sec:by-checker}

\input{table-checkers}

We also broke out the results by checker, in \cref{tab:checkers}. \todo{Discuss
  the results, especially if there are significant differences between checkers..}

\todo{Also emphasize the absolute number of checkers that WPI applies to.}
