\section{Practical Considerations}
\label{sec:difficulties}

The algorithm and type rules in \cref{sec:algorithm} define our
basic approach. However, there are also some other challenges
that need to be addressed in order to build a system that applies
to every pluggable typechecker and is effective at inferring
type qualifiers for real programs. This section describes the
challenges we encountered while building a tool that implements
the algorithm in \cref{sec:algorithm} and our solutions to those
challenges.

\subsection{Analysis Termination: Infinite Descending Chains and Recursive Calls}
\label{sec:infinite-descending-chains}

The usual lattice definition in an abstract interpretation
(or equivalently, a type system with flow-sensitive refinement like our
pluggable typecheckers~\cite{Cousot1997}) forbids
infinite ascending chains but permits infinite descending chains.
A typechecker whose type system includes an infinite descending chain
is guaranteed to terminate when typechecking a program, but our
inference algorithm can induce downward refinement.

For example, consider a type system for integer intervals
whose type qualifiers are of the form \<@Int\-Range(from, to)>.
The type \<@Int\-Range(1, 10) int> therefore means ``an integer between
1 and 10, inclusive'' when running this pluggable typechecker.
The subtyping relationship is straightforward: \<@Int\-Range(a, z)>
is a supertype of \<@Int\-Range(b, y)> if and only if \codeid{a < b \&\& z > y}.
Then consider running our inference algorithm on the program in \cref{fig:descend}
using this pluggable typechecker.

\begin{figure}
\begin{verbatim}
int recurse(int x) {
  return recurse(x + 1);
}

recurse(0);
\end{verbatim}
\caption{Simple example which would trigger an infinite loop in a naive
  implementation of our analysis.}
\label{fig:descend}
\end{figure}

When typechecking this program, the pluggable typechecker will
treat both the input and output of \<recurse> as the top type
(\ie any integer). However, the invocation rule in \todo{rules figure}
results in the fixpoint algorithm getting stuck in an arbitrarily-long
loop attempting to infer a type for the parameter \<x>: the call to \<recurse(0)>
results in a qualified type of \<@IntRange(0, 0)> for \<x>. Constant
propagation then leads to the type \<@IntRange(0, 1)> in the next iteration
of the overall fixpoint loop, then to \<@IntRange(0, 2)>, \ldots, which
is undesirable.

In practice, we found that a simple modification to the invocation rule that
prevents type qualifier inference from recursive method calls is sufficient
to prevent this problem in practice.\todo{Mention that a narrowing
  approach, analogous to widening that is already used in dataflow/abstract
  interpretation, would also be a possible solution.}
 Though mutual recursion is a problem in
theory, in our benchmarks we did not encounter any code that triggered such
an infinite loop in the analysis. Future work should address this problem in
a more principled way.

\subsection{Pre- and Post-conditions}
\label{sec:pre-post-conditions}

Discuss some of the troubles that Mike encountered when
he implemented pre- and post-condition support in WPI. Frame
this as a theoretical problem. \todo{Mike should write this section.}

\subsection{Output Format}
\label{sec:output}

\todo{This was a big engineering challenge, but I'm not sure if
  it deserves this much space in the paper. Maybe cut it down to one
  paragraph, move it to the implementation section, or both? Or
  maybe cut it all together?}

\Cref{alg:wpi-fixpoint} does not specify how to store the intermediate
sets of candidate type qualfiers (\ie $A^{\prime}$). Choosing the right way
to do so is a surprisingly difficult engineering task. This section describes some
of the pitfalls and how we overcame them.

An obvious solution (and the first we tried) is to store the annotations
in the program itself, as if they had been written by the programmer. The
problem with this strategy is that it becomes impossible to distinguish
between qualifiers written by the programmer themselves and qualifiers
inferred in previous fixpoint rounds. Because the programmer might write
qualifiers themselves to prevent the inference system from making overly-precise
refinements (which would result in type errors in \eg other modules not currently
under compilation), the analysis should not further refine any qualifiers
provided by the programmer.

Our second approach was to store the type qualifiers in a side file, using
the standard approach for providing specifications for
libraries that the code under compilation depends on, but which were not
themselves compiled with the pluggable typechecker (and therefore whose bytecode
does not contain type qualifiers). The downside of this approach is that
the requirements for ``library qualifier'' side files and inferred side files
are quite different. Consider the case where a library qualifier side file and
a source file exist for the same class. In this case, the sensible default is
to \emph{ignore} the library qualifier file, because the source code is actually
the ground truth. This problem is compounded by typechecker writers distributing
library qualifier files for common libraries. We built a switch that would force
all library qualifier files to be placed into source code, but this made inference
not work properly on common libraries for which checker-distributed library qualifier files
existed.

Our third (and final) approach was to build an inference-specific side file mechanism.
Though this required our implementation to be larger, it did not have any of the problems
described above.
